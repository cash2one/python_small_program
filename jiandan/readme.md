#看我看我  快看我
本来打算学习使用scrapy的 但是   还是 因为   python3  和 scrapy的兼容性问题   先放弃  这个程序  自己先写吧   就这样
大概完成了,主要分成两部分  就是 抓取 页面 的图片 链接   放入 数据库  然后  再 一次  把 链接  下载 到本地  10g+  比我想得要 多  :)
##注意两点 
    1.多任务的时候 必须使用多线程了   单线程  慢的要死
    2.必须找一个  能够 稳定运行 js 的爬虫 框架 了
##记录
    煎蛋网站的反爬虫策略炒鸡简单(怪不得大家都拿这个网站 做爬虫练习  不过 讲真  这个比 美团 要好)  就是注意添加 浏览器 表头的所有内容就行了.
    还有煎蛋上边 的妹子 的质量  真的不高,回头看  花瓣上的妹子图片才是 那种小鹿乱撞的赶脚
    照片的话上传到  百度网盘  然后   分享出去 
    下一个目标  花瓣   go
##后来
    do_file.py是对文件的处理 主要是 因为 百度网盘 的限制   上传 文件 1000的限制   现在 分为  46个文件夹  和压缩包   上传到百度云